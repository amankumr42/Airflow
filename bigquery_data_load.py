# -*- coding: utf-8 -*-
"""bigquery_data_load.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173Fpo15KabjhhoXxFGjovBwLvi3LUEPD
"""

#! pip install apache-airflow

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator
from airflow.contrib.operators.bigquery_operator import BigQueryOperator

default_arguments = {
    'owner' : 'Aman Kumar',
    'start_date' : days_ago(1)
}

with DAG('bigquery_data_load', schedule_interval='@hourly', catchup=False, 
         default_args=default_arguments) as dag:
           load_data = GoogleCloudStorageToBigQueryOperator(
               task_id = 'load_data',
               bucket= 'aa-logistics-landing-bucket-test1',
               source_objects=['*'],
               source_format='CSV',
               skip_leading_rows=1,
               field_delimiter=',',
               destination_project_dataset_table= '<project_id>.demos.history',
               create_disposition='CREATE_IF_NEEDED',
               write_disposition='WRITE_APPEND',
               bigquery_conn_id='google_cloud_default',
               google_cloud_storage_conn_id='google_cloud_default'
           )

           query = """
           SELECT * except (rank)
           FROM (
             SELECT
                *,
                ROW_NUMBER() OVER (
                    PARTITION BY vehicle_id ORDER BY DATETIME(date, TIME(hour, minute, 0)) DESC
                ) as rank
             FROM `<project_id>.demos.history`) as latest
           WHERE rank = 1;
           """
          
           create_table =  BigQueryOperator(
              task_id='create_table',
              sql=query,
              destination_project_dataset_table='<project_id>.demos.latest',
              write_disposition='WRITE_TRUNCATE',
              create_disposition='CREATE_IF_NEEDED',
              use_legacy_sql=False,
              location='us-east1',
              bigquery_conn_id='google_cloud_default'
           )

load_data >> create_table



\